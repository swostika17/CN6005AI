{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swostika17/CN6005AI/blob/main/week8_lab_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Calculate Imformation Gain for each feature Setp1:Calculate Entropy for thr entire dataset Total:10 Yes:5 No:5 E(D)=-(0.5log(0.5)+0.5log(0.5))=-(-1)=1 Step2: Calculate entropy for the Weather Condition 2.1E(Weather Condition=Rain) Total 3, Yes:1, No:2 E(Weather Condition=Rain)=-(0.34log(0.34)+0.68lpg(0.68)=0.918 2.2E(Weather Contidion= Snow) Total:34, Yes:2, NO:1 E(Weather Condition=Snow) Total:3,Yes:2, NO:1 E(Weather Condition Snow)= -(0.68log(0.68)+0.34log(0.34))=)0.918 2.3E(Weather Condition=Clear) Total 4, Yes:2, No:2 E(Weather condition clear)=-(0.5log(0.5)+0.5log(0.5)=1 2.4 Average Weight Entropy of the weather condition E(Weather condition)=0.340.9180+0.340.918+0.251=0.951 Step 3 : information Gain for the Weather Condition IG(Weather condition)=E(D)-E(Weather Condition)=1-0.951=0.049 2.5 (Traffic Condition= High) Total= 4 Yes:2, No:2 E(Traffic Condition=High)=-(0.25log(0.25)+0.25log(0.25)) = 2.6 (Traffic Condition= Normal) Total:3 Yes:1 No:1 (Traffic Condition= Normal)=-(0.33log(0.33)+0.67log(0.67))\n",
        "\n",
        "2.7E(Traffic Condition Light)= Total:3 Yes:1, No:2 E(Traffic Condition Light)=-(-0.33log(0.33)+0.67log(0.67))=\n",
        "\n",
        "2.8 E( Engine Problem) total : 10 E(Engine problem=no) Total=6 No=4 Yes=2 E(Engine Problem=no)=-(0.33log(0.6)+0.4log(0.4))= E(Engine Problem = yes) total=4 yes=3, no=1 E(Engine problem= No)=-(0.75log(0.75)+0.25log(0.25))=\n",
        "\n"
      ],
      "metadata": {
        "id": "JJCidRdfHWnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import log2\n",
        "\n",
        "# -----------------------------\n",
        "# 1. LOAD YOUR DATA\n",
        "# -----------------------------\n",
        "data = pd.DataFrame({\n",
        "    'Outlook': ['Sunny','Sunny','Overcast','Rain','Rain','Rain','Overcast','Sunny'],\n",
        "    'Wind':    ['Weak','Strong','Weak','Weak','Strong','Weak','Strong','Weak'],\n",
        "    'Play':    ['No','No','Yes','Yes','No','Yes','Yes','Yes']\n",
        "})\n",
        "\n",
        "target = 'Play'\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2. ENTROPY FUNCTION\n",
        "# -----------------------------\n",
        "def entropy(column):\n",
        "    values, counts = np.unique(column, return_counts=True)\n",
        "    total = sum(counts)\n",
        "    ent = 0\n",
        "    for i in range(len(values)):\n",
        "        p = counts[i] / total\n",
        "        ent += -p * log2(p)\n",
        "    return ent\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3. ENTROPY AFTER SPLIT\n",
        "# -----------------------------\n",
        "def entropy_split(data, attribute, target):\n",
        "    values, counts = np.unique(data[attribute], return_counts=True)\n",
        "    total = sum(counts)\n",
        "    weighted_entropy = 0\n",
        "\n",
        "    for i in range(len(values)):\n",
        "        subset = data[data[attribute] == values[i]]\n",
        "        weighted_entropy += (counts[i] / total) * entropy(subset[target])\n",
        "\n",
        "    return weighted_entropy\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 4. INFORMATION GAIN\n",
        "# -----------------------------\n",
        "def info_gain(data, attribute, target):\n",
        "    total_entropy = entropy(data[target])\n",
        "    return total_entropy - entropy_split(data, attribute, target)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 5. GINI IMPURITY\n",
        "# -----------------------------\n",
        "def gini(column):\n",
        "    values, counts = np.unique(column, return_counts=True)\n",
        "    total = sum(counts)\n",
        "    g = 1 - sum((counts[i] / total) ** 2 for i in range(len(values)))\n",
        "    return g\n",
        "\n",
        "\n",
        "def gini_split(data, attribute, target):\n",
        "    values, counts = np.unique(data[attribute], return_counts=True)\n",
        "    total = sum(counts)\n",
        "    weighted_gini = 0\n",
        "\n",
        "    for i in range(len(values)):\n",
        "        subset = data[data[attribute] == values[i]]\n",
        "        weighted_gini += (counts[i] / total) * gini(subset[target])\n",
        "\n",
        "    return weighted_gini\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 6. PRINT RESULTS STEP-BY-STEP\n",
        "# -----------------------------\n",
        "\n",
        "print(\"\\n==============================\")\n",
        "print(\"STEP 1: TOTAL ENTROPY\")\n",
        "print(\"==============================\")\n",
        "print(\"Entropy of Play =\", entropy(data[target]))\n",
        "\n",
        "\n",
        "print(\"\\n==============================\")\n",
        "print(\"STEP 2: ENTROPY AFTER SPLIT\")\n",
        "print(\"==============================\")\n",
        "for col in data.columns:\n",
        "    if col != target:\n",
        "        print(f\"{col}: {entropy_split(data, col, target)}\")\n",
        "\n",
        "\n",
        "print(\"\\n==============================\")\n",
        "print(\"STEP 3: INFORMATION GAIN\")\n",
        "print(\"==============================\")\n",
        "info_gains = {}\n",
        "for col in data.columns:\n",
        "    if col != target:\n",
        "        ig = info_gain(data, col, target)\n",
        "        info_gains[col] = ig\n",
        "        print(f\"{col}: {ig}\")\n",
        "\n",
        "best_entropy_attribute = max(info_gains, key=info_gains.get)\n",
        "print(\"\\nBest attribute using Entropy =\", best_entropy_attribute)\n",
        "\n",
        "\n",
        "print(\"\\n==============================\")\n",
        "print(\"STEP 4: GINI IMPURITY AFTER SPLIT\")\n",
        "print(\"==============================\")\n",
        "gini_values = {}\n",
        "for col in data.columns:\n",
        "    if col != target:\n",
        "        g = gini_split(data, col, target)\n",
        "        gini_values[col] = g\n",
        "        print(f\"{col}: {g}\")\n",
        "\n",
        "best_gini_attribute = min(gini_values, key=gini_values.get)\n",
        "print(\"\\nBest attribute using Gini =\", best_gini_attribute)\n",
        "\n",
        "\n",
        "# -------------------------------------\n",
        "# 5. PRINT RESULTS\n",
        "# -------------------------------------\n",
        "print(\"=== ENTROPY OF TARGET ===\")\n",
        "print(entropy(data[target]))\n",
        "\n",
        "print(\"\\n=== INFORMATION GAIN ===\")\n",
        "for col in data.columns:\n",
        "    if col != target:\n",
        "        print(f\"{col}: {info_gain(data, col, target)}\")\n",
        "\n",
        "print(\"\\n=== GINI IMPURITY FOR SPLITS ===\")\n",
        "for col in data.columns:\n",
        "    if col != target:\n",
        "        print(f\"{col}: {gini_split(data, col, target)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yddCBSzH7C4-",
        "outputId": "306ae8f7-f579-4bab-aa9e-34541648183e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "STEP 1: TOTAL ENTROPY\n",
            "==============================\n",
            "Entropy of Play = 0.954434002924965\n",
            "\n",
            "==============================\n",
            "STEP 2: ENTROPY AFTER SPLIT\n",
            "==============================\n",
            "Outlook: 0.6887218755408672\n",
            "Wind: 0.7955659970750351\n",
            "\n",
            "==============================\n",
            "STEP 3: INFORMATION GAIN\n",
            "==============================\n",
            "Outlook: 0.2657121273840979\n",
            "Wind: 0.15886800584993\n",
            "\n",
            "Best attribute using Entropy = Outlook\n",
            "\n",
            "==============================\n",
            "STEP 4: GINI IMPURITY AFTER SPLIT\n",
            "==============================\n",
            "Outlook: 0.3333333333333333\n",
            "Wind: 0.3666666666666666\n",
            "\n",
            "Best attribute using Gini = Outlook\n",
            "=== ENTROPY OF TARGET ===\n",
            "0.954434002924965\n",
            "\n",
            "=== INFORMATION GAIN ===\n",
            "Outlook: 0.2657121273840979\n",
            "Wind: 0.15886800584993\n",
            "\n",
            "=== GINI IMPURITY FOR SPLITS ===\n",
            "Outlook: 0.3333333333333333\n",
            "Wind: 0.3666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x0sKgtF_smCD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}